import urllib.request as libreq
import xml.etree.ElementTree as ET
import datetime
from urllib.parse import quote
import os

def demonstrate_daily_arxiv_feed():
    """æ¼”ç¤ºå®Œæ•´çš„æ¯æ—¥arXivè®¢é˜…åŠŸèƒ½"""
    
    print("ğŸ¯ arXivæ¯æ—¥è®ºæ–‡è®¢é˜…ç³»ç»Ÿ - å®Œæ•´æ¼”ç¤º")
    print("=" * 60)
    
    # æµ‹è¯•ä¸€ä¸ªå·²çŸ¥æœ‰è®ºæ–‡çš„æ—¥æœŸ
    test_date = "20241120"  # å‡ å¤©å‰ï¼Œç¡®ä¿æœ‰æ•°æ®
    date_range = f"[{test_date} TO {test_date}]"
    
    print(f"\nğŸ“… è·å– {test_date} çš„æ–°è®ºæ–‡:")
    print("-" * 40)
    
    # 1. è·å–æ‰€æœ‰åˆ†ç±»çš„æ–°è®ºæ–‡
    search_query = f"submittedDate:{date_range}"
    encoded_query = quote(search_query, safe='')
    
    api_url = (
        f"http://export.arxiv.org/api/query?"
        f"search_query={encoded_query}&"
        f"sortBy=submittedDate&"
        f"sortOrder=descending&"
        f"max_results=10"
    )
    
    print(f"ğŸ”— API URL: {api_url}")
    
    try:
        with libreq.urlopen(api_url) as url:
            response = url.read()
        
        root = ET.fromstring(response)
        ns = {'atom': 'http://www.w3.org/2005/Atom'}
        
        # æ£€æŸ¥æ€»ç»“æœæ•°
        total_results_elem = root.find('.//{http://a9.com/-/spec/opensearch/1.1/}totalResults')
        total_results = int(total_results_elem.text) if total_results_elem is not None else 0
        
        papers = root.findall('atom:entry', ns)
        
        print(f"ğŸ“Š ç»Ÿè®¡: æ€»å…± {total_results} ç¯‡è®ºæ–‡ï¼Œè¿”å› {len(papers)} ç¯‡")
        print("\nğŸ“š è®ºæ–‡åˆ—è¡¨:")
        
        for i, paper in enumerate(papers, 1):
            title = paper.find('atom:title', ns).text.strip() if paper.find('atom:title', ns) is not None else "Unknown"
            published = paper.find('atom:published', ns).text if paper.find('atom:published', ns) is not None else "Unknown"
            
            # æå–åˆ†ç±»
            categories = []
            for cat in paper.findall('atom:category', ns):
                categories.append(cat.get('term'))
            
            # æå–ä½œè€…
            authors = []
            for author in paper.findall('atom:author/atom:name', ns):
                authors.append(author.text)
            
            # æŸ¥æ‰¾PDFé“¾æ¥
            pdf_url = None
            for link in paper.findall('atom:link', ns):
                if link.get('title') == 'pdf':
                    pdf_url = link.get('href')
                    break
            
            print(f"\n[{i}] {title}")
            print(f"   ä½œè€…: {', '.join(authors[:2])}{'...' if len(authors) > 2 else ''}")
            print(f"   åˆ†ç±»: {', '.join(categories[:2])}{'...' if len(categories) > 2 else ''}")
            print(f"   å‘å¸ƒæ—¶é—´: {published}")
            if pdf_url:
                print(f"   ğŸ“„ PDFå¯ç”¨: {pdf_url}")
        
        # 2. æ¼”ç¤ºç‰¹å®šåˆ†ç±»çš„æœç´¢
        print(f"\n" + "="*60)
        print("ğŸ”¬ æŒ‰åˆ†ç±»æœç´¢ (è®¡ç®—æœºç§‘å­¦-äººå·¥æ™ºèƒ½):")
        print("-" * 40)
        
        category_search_query = f"cat:cs.AI+AND+submittedDate:{date_range}"
        encoded_category_query = quote(category_search_query, safe='')
        
        category_api_url = (
            f"http://export.arxiv.org/api/query?"
            f"search_query={encoded_category_query}&"
            f"sortBy=submittedDate&"
            f"sortOrder=descending&"
            f"max_results=5"
        )
        
        print(f"ğŸ”— API URL: {category_api_url}")
        
        with libreq.urlopen(category_api_url) as url:
            category_response = url.read()
        
        category_root = ET.fromstring(category_response)
        category_papers = category_root.findall('atom:entry', ns)
        
        category_total_elem = category_root.find('.//{http://a9.com/-/spec/opensearch/1.1/}totalResults')
        category_total = int(category_total_elem.text) if category_total_elem is not None else 0
        
        print(f"ğŸ“Š ç»Ÿè®¡: æ€»å…± {category_total} ç¯‡AIè®ºæ–‡ï¼Œè¿”å› {len(category_papers)} ç¯‡")
        
        for i, paper in enumerate(category_papers, 1):
            title = paper.find('atom:title', ns).text.strip() if paper.find('atom:title', ns) is not None else "Unknown"
            print(f"   {i}. {title[:70]}...")
        
        # 3. æ¼”ç¤ºè®ºæ–‡ä¸‹è½½åŠŸèƒ½
        print(f"\n" + "="*60)
        print("ğŸ’¾ è®ºæ–‡ä¸‹è½½æ¼”ç¤º:")
        print("-" * 40)
        
        download_dir = "demo_papers"
        if not os.path.exists(download_dir):
            os.makedirs(download_dir)
        
        # ä¸‹è½½ç¬¬ä¸€ç¯‡è®ºæ–‡
        if papers:
            first_paper = papers[0]
            pdf_url = None
            for link in first_paper.findall('atom:link', ns):
                if link.get('title') == 'pdf':
                    pdf_url = link.get('href')
                    break
            
            if pdf_url:
                title = first_paper.find('atom:title', ns).text.strip()
                print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½: {title[:50]}...")
                
                # åˆ›å»ºæ–‡ä»¶å
                year = first_paper.find('atom:published', ns).text[:4] if first_paper.find('atom:published', ns) is not None else "unknown"
                title_words = title.split()[:3]
                clean_title = "_".join(title_words).lower()
                filename = f"{year}_{clean_title}.pdf"
                
                # æ¸…ç†æ–‡ä»¶å
                invalid_chars = '<>:"/\\|?*'
                for char in invalid_chars:
                    filename = filename.replace(char, '_')
                
                filepath = os.path.join(download_dir, filename)
                
                try:
                    libreq.urlretrieve(pdf_url, filepath)
                    print(f"âœ… ä¸‹è½½å®Œæˆ: {filename}")
                    print(f"ğŸ“ ä¿å­˜ä½ç½®: {download_dir}/")
                except Exception as e:
                    print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
            else:
                print("âŒ ç¬¬ä¸€ç¯‡è®ºæ–‡æ— PDFé“¾æ¥")
        else:
            print("âŒ æ— è®ºæ–‡å¯ä¸‹è½½")
        
        print(f"\n" + "="*60)
        print("ğŸ‰ æ¼”ç¤ºå®Œæˆ!")
        print("\nğŸ“‹ æ€»ç»“:")
        print("   âœ… å¯ä»¥è·å–æ¯æ—¥æ–°è®ºæ–‡")
        print("   âœ… æ”¯æŒæŒ‰åˆ†ç±»ç­›é€‰")
        print("   âœ… å¯ä»¥ä¸‹è½½PDFè®ºæ–‡")
        print("   âœ… æ”¯æŒæ—¥æœŸèŒƒå›´æœç´¢")
        print("   âœ… å¯ä»¥æŒ‰æäº¤æ—¶é—´æ’åº")
        print(f"\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
        print("   1. æ¯å¤©è¿è¡Œä¸€æ¬¡è·å–æœ€æ–°è®ºæ–‡")
        print("   2. è®¾ç½®æ„Ÿå…´è¶£çš„åˆ†ç±»è¿‡æ»¤")
        print("   3. è‡ªåŠ¨ä¸‹è½½æ„Ÿå…´è¶£çš„è®ºæ–‡")
        print("   4. å¯ä»¥é›†æˆåˆ°é‚®ä»¶é€šçŸ¥ç³»ç»Ÿ")
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")

if __name__ == "__main__":
    demonstrate_daily_arxiv_feed()