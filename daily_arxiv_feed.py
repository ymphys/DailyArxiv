import urllib.request as libreq
import xml.etree.ElementTree as ET
import datetime
import time
import os

class DailyArxivFeed:
    def __init__(self, download_dir="daily_papers"):
        self.download_dir = download_dir
        if not os.path.exists(download_dir):
            os.makedirs(download_dir)
    
    def get_date_range(self, days_back=0):
        """è·å–æ—¥æœŸèŒƒå›´"""
        today = datetime.datetime.now()
        target_date = today - datetime.timedelta(days=days_back)
        date_str = target_date.strftime("%Y%m%d")
        return f"[{date_str} TO {date_str}]"
    
    def fetch_daily_papers(self, category=None, max_results=25, days_back=0):
        """è·å–æ¯æ—¥æ–°è®ºæ–‡"""
        # æ„å»ºåŸºç¡€æŸ¥è¯¢
        date_range = self.get_date_range(days_back)
        base_query = f"submittedDate:{date_range}"
        
        # å¦‚æœæŒ‡å®šäº†åˆ†ç±»ï¼Œæ·»åŠ åˆ°æŸ¥è¯¢ä¸­
        if category:
            search_query = f"cat:{category}+AND+{base_query}"
        else:
            search_query = base_query
        
        # æ„å»ºAPI URL - ä½¿ç”¨urllib.parseæ¥æ­£ç¡®ç¼–ç URL
        from urllib.parse import quote
        
        # æ­£ç¡®ç¼–ç æŸ¥è¯¢å‚æ•°
        encoded_query = quote(search_query, safe='')
        api_url = (
            f"http://export.arxiv.org/api/query?"
            f"search_query={encoded_query}&"
            f"sortBy=submittedDate&"
            f"sortOrder=descending&"
            f"max_results={max_results}"
        )
        
        print(f"ğŸ” æŸ¥è¯¢æ¯æ—¥arXivè®ºæ–‡")
        print(f"   æ—¥æœŸèŒƒå›´: {date_range}")
        if category:
            print(f"   åˆ†ç±»: {category}")
        print(f"   æœ€å¤§ç»“æœ: {max_results}")
        print(f"   API URL: {api_url}")
        print("=" * 60)
        
        try:
            # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(1)
            
            with libreq.urlopen(api_url) as url:
                response = url.read()
            
            return self.parse_response(response, date_range, category)
            
        except Exception as e:
            print(f"âŒ è·å–å¤±è´¥: {e}")
            return None
    
    def parse_response(self, xml_content, date_range, category):
        """è§£æAPIå“åº”"""
        try:
            root = ET.fromstring(xml_content)
            ns = {'atom': 'http://www.w3.org/2005/Atom'}
            
            # æ£€æŸ¥æ€»ç»“æœæ•°
            total_results_elem = root.find('.//{http://a9.com/-/spec/opensearch/1.1/}totalResults')
            total_results = int(total_results_elem.text) if total_results_elem is not None else 0
            
            papers = []
            for entry in root.findall('atom:entry', ns):
                paper_info = {
                    'title': entry.find('atom:title', ns).text.strip() if entry.find('atom:title', ns) is not None else "Unknown Title",
                    'id': entry.find('atom:id', ns).text,
                    'summary': entry.find('atom:summary', ns).text.strip() if entry.find('atom:summary', ns) is not None else "",
                    'published': entry.find('atom:published', ns).text if entry.find('atom:published', ns) is not None else "",
                    'updated': entry.find('atom:updated', ns).text if entry.find('atom:updated', ns) is not None else "",
                    'pdf_url': None,
                    'authors': []
                }
                
                # æå–ä½œè€…
                for author in entry.findall('atom:author/atom:name', ns):
                    paper_info['authors'].append(author.text)
                
                # æå–åˆ†ç±»
                paper_info['categories'] = []
                for cat in entry.findall('atom:category', ns):
                    paper_info['categories'].append(cat.get('term'))
                
                # æŸ¥æ‰¾PDFé“¾æ¥
                for link in entry.findall('atom:link', ns):
                    if link.get('title') == 'pdf':
                        paper_info['pdf_url'] = link.get('href')
                        break
                
                papers.append(paper_info)
            
            return {
                'total_results': total_results,
                'papers': papers,
                'date_range': date_range,
                'category': category
            }
            
        except Exception as e:
            print(f"âŒ è§£æå¤±è´¥: {e}")
            return None
    
    def display_results(self, results):
        """æ˜¾ç¤ºç»“æœ"""
        if not results:
            print("âŒ æ— ç»“æœ")
            return
        
        print(f"\nğŸ“Š æœç´¢ç»“æœ:")
        print(f"   æ€»å…±æ‰¾åˆ°: {results['total_results']} ç¯‡è®ºæ–‡")
        print(f"   è¿”å›äº†: {len(results['papers'])} ç¯‡è®ºæ–‡")
        print(f"   æ—¥æœŸèŒƒå›´: {results['date_range']}")
        if results['category']:
            print(f"   åˆ†ç±»: {results['category']}")
        print("-" * 60)
        
        for i, paper in enumerate(results['papers'], 1):
            print(f"\n[{i}] {paper['title']}")
            print(f"   ä½œè€…: {', '.join(paper['authors'][:3])}{'...' if len(paper['authors']) > 3 else ''}")
            print(f"   å‘å¸ƒæ—¶é—´: {paper['published']}")
            print(f"   åˆ†ç±»: {', '.join(paper['categories'][:2])}{'...' if len(paper['categories']) > 2 else ''}")
            if paper['pdf_url']:
                print(f"   PDF: {paper['pdf_url']}")
            
            # æ˜¾ç¤ºæ‘˜è¦å‰100ä¸ªå­—ç¬¦
            if paper['summary']:
                summary_preview = paper['summary'][:100] + "..." if len(paper['summary']) > 100 else paper['summary']
                print(f"   æ‘˜è¦: {summary_preview}")
    
    def download_selected_papers(self, results, indices=None):
        """ä¸‹è½½é€‰å®šçš„è®ºæ–‡"""
        if not results or not results['papers']:
            print("âŒ æ— è®ºæ–‡å¯ä¸‹è½½")
            return
        
        if indices is None:
            indices = range(len(results['papers']))
        
        downloaded_count = 0
        for idx in indices:
            if 0 <= idx < len(results['papers']):
                paper = results['papers'][idx]
                if paper['pdf_url']:
                    if self.download_paper(paper):
                        downloaded_count += 1
        
        print(f"\nâœ… ä¸‹è½½å®Œæˆ: {downloaded_count} ç¯‡è®ºæ–‡")
    
    def download_paper(self, paper_info):
        """ä¸‹è½½å•ä¸ªè®ºæ–‡PDF"""
        if not paper_info['pdf_url']:
            print(f"âœ— è·³è¿‡ '{paper_info['title']}' - æ— PDFé“¾æ¥")
            return False
        
        try:
            # åˆ›å»ºæ›´å¥½çš„æ–‡ä»¶å
            year = paper_info['published'][:4] if paper_info['published'] else "unknown"
            title_words = paper_info['title'].split()[:4]
            clean_title = "_".join(title_words).lower()
            filename = f"{year}_{clean_title}.pdf"
            
            # æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦
            invalid_chars = '<>:"/\\|?*'
            for char in invalid_chars:
                filename = filename.replace(char, '_')
            
            filepath = os.path.join(self.download_dir, filename)
            
            print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½: {paper_info['title']}")
            libreq.urlretrieve(paper_info['pdf_url'], filepath)
            print(f"   âœ… ä¿å­˜ä¸º: {filename}")
            return True
            
        except Exception as e:
            print(f"   âŒ ä¸‹è½½å¤±è´¥: {e}")
            return False

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºæ¯æ—¥arXivè®¢é˜…åŠŸèƒ½"""
    feed = DailyArxivFeed()
    
    print("ğŸš€ arXivæ¯æ—¥è®ºæ–‡è®¢é˜…ç³»ç»Ÿ")
    print("=" * 50)
    
    # ç¤ºä¾‹ï¼šè·å–ä»Šå¤©çš„æ–°è®ºæ–‡
    print("\n1. è·å–ä»Šå¤©çš„æ–°è®ºæ–‡:")
    today_results = feed.fetch_daily_papers(max_results=10, days_back=0)
    feed.display_results(today_results)
    
    # ç¤ºä¾‹ï¼šè·å–ç‰¹å®šåˆ†ç±»çš„æ–°è®ºæ–‡
    print("\n" + "="*60)
    print("2. è·å–è®¡ç®—æœºç§‘å­¦-äººå·¥æ™ºèƒ½åˆ†ç±»çš„æ–°è®ºæ–‡:")
    cs_ai_results = feed.fetch_daily_papers(category="cs.AI", max_results=5, days_back=0)
    feed.display_results(cs_ai_results)
    
    # ç¤ºä¾‹ï¼šè·å–æ˜¨å¤©çš„è®ºæ–‡
    print("\n" + "="*60)
    print("3. è·å–æ˜¨å¤©çš„è®ºæ–‡:")
    yesterday_results = feed.fetch_daily_papers(max_results=5, days_back=1)
    feed.display_results(yesterday_results)
    
    # å¯é€‰ï¼šä¸‹è½½ä¸€äº›è®ºæ–‡
    print("\n" + "="*60)
    print("4. ä¸‹è½½å‰2ç¯‡è®ºæ–‡:")
    if today_results and today_results['papers']:
        feed.download_selected_papers(today_results, indices=[0, 1])

if __name__ == "__main__":
    main()