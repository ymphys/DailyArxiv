# arXiv stat 今日论文 (2025-11-24)

共找到 63 篇论文

## 1. Regularized Reduced Rank Regression for mixed predictor and response variables

**作者**: Lorenza Cotugno, Mark de Rooij, Roberta Siciliano

**PDF链接**: [https://arxiv.org/pdf/2511.16718.pdf](https://arxiv.org/pdf/2511.16718.pdf)

**摘要**: In this paper, we introduce the Generalized Mixed Regularized Reduced Rank Regression model (GMR4), an extension of the GMR3 model designed to improve performance in high-dimensional settings. GMR3 is a regression method for a mix of numeric, binary and ordinal response variables, while also allowing for mixed-type predictors through optimal scaling. GMR4 extends this approach by incorporating regularization techniques, such as Ridge, Lasso, Group Lasso, or any combination thereof, making the model suitable for datasets with a large number of predictors or collinearity among them. In addition, we propose a cross-validation procedure that enables the estimation of the rank S and the penalty parameter lambda. Through a simulation study, we evaluate the performance of the model under different scenarios, varying the sample size, the number of non-informative predictors and response dimension. The results of the simulation study guide the choice of the penalty parameter lambda in the empirical application ISSP: Health and Healthcare I-II (2023), which includes mixed-type predictors and ordinal responses. In this application, the model results in a sparse and interpretable solution, with a limited set of influential predictors that provide insights into public attitudes toward healthcare.

---

## 2. Correlation Matters! Streamlining the Sample Size Procedure with Composite Time-to-event Endpoints

**作者**: Yunhan Mou, Fan Li, Denise Esserman, Yuan Huang

**PDF链接**: [https://arxiv.org/pdf/2511.16773.pdf](https://arxiv.org/pdf/2511.16773.pdf)

**摘要**: Composite endpoints are widely used in cardiovascular clinical trials to improve statistical efficiency while preserving clinical relevance. The Win Ratio (WR) measure and more general frameworks of Win Statistics have emerged as increasingly popular alternatives to traditional time-to-first-event analyses. Although analytic sample size formulas for WR have been developed, they rely on design parameters that are often not straightforward to specify. Consequently, sample size determination in clinical trials with WR as the primary analysis is most often based on simulations, which can be computationally intensive. Moreover, these simulations commonly assume independence among component endpoints, an assumption that may not hold in practice and can lead to misleading power estimates. To address this challenge, we derive refined formulas to calculate the proportions of wins, losses, and ties for multiple prioritized time-to-event endpoints. These formulas rely on familiar design inputs and become directly applicable when integrated with existing sample size methods. We conduct a comprehensive assessment of how correlation among endpoints affects sample size requirements across varying design features. We further demonstrate the role of correlations through two case studies based on the landmark SPRINT and STICH clinical trials to generate further insights.

---

## 3. BITS for GAPS: Bayesian Information-Theoretic Sampling for hierarchical GAussian Process Surrogates

**作者**: Kyla D. Jones, Alexander W. Dowling

**PDF链接**: [https://arxiv.org/pdf/2511.16815.pdf](https://arxiv.org/pdf/2511.16815.pdf)

**摘要**: We introduce the Bayesian Information-Theoretic Sampling for hierarchical GAussian Process Surrogates (BITS for GAPS) framework to emulate latent components in hybrid physical systems. BITS for GAPS supports serial hybrid modeling, where known physics governs part of the system and residual dynamics are represented as a latent function inferred from data. A Gaussian process prior is placed over the latent function, with hierarchical priors on its hyperparameters to encode physically meaningful structure in the predictive posterior.
To guide data acquisition, we derive entropy-based acquisition functions that quantify expected information gain from candidate input locations, identifying samples most informative for training the surrogate. Specifically, we obtain a closed-form expression for the differential entropy of the predictive posterior and establish a tractable lower bound for efficient evaluation. These derivations approximate the predictive posterior as a finite, uniformly weighted mixture of Gaussian processes.
We demonstrate the framework's utility by modeling activity coefficients in vapor-liquid equilibrium systems, embedding the surrogate into extended Raoult's law for distillation design. Numerical results show that entropy-guided sampling improves sample efficiency by targeting regions of high uncertainty and potential information gain. This accelerates surrogate convergence, enhances predictive accuracy in non-ideal regimes, and preserves physical consistency. Overall, BITS for GAPS provides an efficient, interpretable, and uncertainty-aware framework for hybrid modeling of complex physical systems.

---

## 4. Trust-Aware Multimodal Data Fusion for Yield Estimation: A Case Study of the 2020 Beirut Explosion

**作者**: Lekha Patel, Craig Ulmer, Stephen J. Verzi, Daniel J. Krofcheck, Indu Manickam, Asmeret Naugle, Jaideep Ray

**PDF链接**: [https://arxiv.org/pdf/2511.16816.pdf](https://arxiv.org/pdf/2511.16816.pdf)

**摘要**: The estimation of explosive yield from heterogeneous observational data presents fundamental challenges in inverse problems, particularly when combining traditional physical measurements with modern artificial intelligence-interpreted modalities. We present a novel Bayesian fractional posterior framework that fuses seismic waves, crater dimensions, synthetic aperture radar imagery, and vision-language model interpreted ground-level images to estimate the yield of the 2020 Beirut explosion. Unlike conventional approaches that may treat data sources equally, our method learns trust weights for each modality through a Dirichlet prior, automatically calibrating the relative information content of disparate observations. Applied to the Beirut explosion, the framework yields an estimate of 0.34--0.48 kt TNT equivalent, representing 12 to 17 percent detonation efficiency relative to the 2.75 kt theoretical maximum from the blast's stored ammonium nitrate. The fractional posterior approach demonstrates superior uncertainty quantification compared to single-modality estimates while providing robustness against systematic biases. This work establishes a principled framework for integrating qualitative assessments with quantitative physical measurements, with applications to explosion monitoring, disaster response, and forensic analysis.

---

## 5. CBMA: Improving conformal prediction through Bayesian model averaging

**作者**: Pankaj Bhagwat, Linglong Kong, Bei Jiang

**PDF链接**: [https://arxiv.org/pdf/2511.16924.pdf](https://arxiv.org/pdf/2511.16924.pdf)

**摘要**: Conformal prediction has emerged as a popular technique for facilitating valid predictive inference across a spectrum of machine learning models, under minimal assumption of exchangeability. Recently, Hoff (2023) showed that full conformal Bayes provides the most efficient prediction sets (smallest by expected volume) among all prediction sets that are valid at the $(1 - \alpha)$ level if the model is correctly specified. However, a critical issue arises when the Bayesian model itself may be mis-specified, resulting in prediction set that might be suboptimal, even though it still enjoys the frequentist coverage guarantee. To address this limitation, we propose an innovative solution that combines Bayesian model averaging (BMA) with conformal prediction. This hybrid not only leverages the strengths of Bayesian conformal prediction but also introduces a layer of robustness through model averaging. Theoretically, we prove that the resulting prediction set will converge to the optimal level of efficiency, if the true model is included among the candidate models. This assurance of optimality, even under potential model uncertainty, provides a significant improvement over existing methods, ensuring more reliable and precise uncertainty quantification.

---

## 6. Optimising pandemic response through vaccination strategies using neural networks

**作者**: Chang Zhai, Ping Chen, Zhuo Jin, David Pitt

**PDF链接**: [https://arxiv.org/pdf/2511.16932.pdf](https://arxiv.org/pdf/2511.16932.pdf)

**摘要**: Epidemic risk assessment poses inherent challenges, with traditional approaches often failing to balance health outcomes and economic constraints. This paper presents a data-driven decision support tool that models epidemiological dynamics and optimises vaccination strategies to control disease spread whilst minimising economic losses. The proposed economic-epidemiological framework comprises three phases: modelling, optimising, and analysing. First, a stochastic compartmental model captures epidemic dynamics. Second, an optimal control problem is formulated to derive vaccination strategies that minimise pandemic-related expenditure. Given the analytical intractability of epidemiological models, neural networks are employed to calibrate parameters and solve the high-dimensional control problem. The framework is demonstrated using COVID-19 data from Victoria, Australia, empirically deriving optimal vaccination strategies that simultaneously minimise disease incidence and governmental expenditure. By employing this three-phase framework, policymakers can adjust input values to reflect evolving transmission dynamics and continuously update strategies, thereby minimising aggregate costs, aiding future pandemic preparedness.

---

## 7. Effects of Distance Metrics and Scaling on the Perturbation Discrimination Score

**作者**: Qiyuan Liu, Qirui Zhang, Jinhong Du, Siming Zhao, Jingshu Wang

**PDF链接**: [https://arxiv.org/pdf/2511.16954.pdf](https://arxiv.org/pdf/2511.16954.pdf)

**摘要**: The Perturbation Discrimination Score (PDS) is increasingly used to evaluate whether predicted perturbation effects remain distinguishable, including in Systema and the Virtual Cell Challenge. However, its behavior in high-dimensional gene-expression settings has not been examined in detail. We show that PDS is highly sensitive to the choice of similarity or distance measure and to the scale of predicted effects. Analysis of observed perturbation responses reveals that $\ell_1$ and $\ell_2$-based PDS behave very differently from cosine-based measures, even after norm matching. We provide geometric insight and discuss implications for future discrimination-based evaluation metrics.

---

## 8. On a synergistic learning phenomenon in nonparametric domain adaptation

**作者**: Ling Zhou, Yuhong Yang

**PDF链接**: [https://arxiv.org/pdf/2511.17009.pdf](https://arxiv.org/pdf/2511.17009.pdf)

**摘要**: Consider nonparametric domain adaptation for regression, which assumes the same conditional distribution of the response given the covariates but different marginal distributions of the covariates. An important goal is to understand how the source data may improve the minimax convergence rate of learning the regression function when the likelihood ratio of the covariate marginal distributions of the target data and the source data are unbounded. A previous work of Pathak et al. (2022) show that the minimax transfer learning rate is simply determined by the faster rate of using either the source or the target data alone. In this paper, we present a new synergistic learning phenomenon (SLP) that the minimax convergence rate based on both data may sometimes be faster (even much faster) than the better rate of convergence based on the source or target data only. The SLP occurs when and only when the target sample size is smaller (in order) than but not too much smaller than the source sample size in relation to the smoothness of the regression function and the nature of the covariate densities of the source and target distributions. Interestingly, the SLP happens in two different ways according to the relationship between the two sample sizes. One is that the target data help alleviate the difficulty in estimating the regression function at points where the density of the source data is close to zero and the other is that the source data (with its larger sample size than that of the target data) help the estimation at points where the density of the source data is not small. Extensions to handle unknown source and target parameters and smoothness of the regression function are also obtained.

---

## 9. Single-Dataset Meta-Analysis For Many-Analysts And Multiverse Studies

**作者**: František Bartoš, Suzanne Hoogeveen, Alexandra Sarafoglou, Samuel Pawel

**PDF链接**: [https://arxiv.org/pdf/2511.17064.pdf](https://arxiv.org/pdf/2511.17064.pdf)

**摘要**: Empirical claims often rely on one population, design, and analysis. Many-analysts, multiverse, and robustness studies expose how results can vary across plausible analytic choices. Synthesizing these results, however, is nontrivial as all results are computed from the same dataset. We introduce single-dataset meta-analysis, a weighted-likelihood approach that incorporates the information in the dataset at most once. It prevents overconfident inferences that would arise if a standard meta-analysis was applied to the data. Single-dataset meta-analysis yields meta-analytic point and interval estimates of the average effect across analytic approaches and of between-analyst heterogeneity, and can be supplied by classical and Bayesian hypothesis tests. Both the common-effect and random-effects versions of the model can be estimated by standard meta-analytic software with small input adjustments. We demonstrate the method via application to the many-analysts study on racial bias in soccer, the many-analysts study of marital status and cardiovascular disease, and the multiverse study on technology use and well-being. The results show how single-dataset meta-analysis complements the qualitative evaluation of many-analysts and multiverse studies.

---

## 10. Shape Analysis of Euclidean Curves under Frenet-Serret Framework

**作者**: Perrine Chassat, Juhyun Park, Nicolas Brunel

**PDF链接**: [https://arxiv.org/pdf/2511.17065.pdf](https://arxiv.org/pdf/2511.17065.pdf)

**摘要**: Geometric frameworks for analyzing curves are common in applications as they focus on invariant features and provide visually satisfying solutions to standard problems such as computing invariant distances, averaging curves, or registering curves. We show that for any smooth curve in R^d, d>1, the generalized curvatures associated with the Frenet-Serret equation can be used to define a Riemannian geometry that takes into account all the geometric features of the shape. This geometry is based on a Square Root Curvature Transform that extends the square root-velocity transform for Euclidean curves (in any dimensions) and provides likely geodesics that avoid artefacts encountered by representations using only first-order geometric information. Our analysis is supported by simulated data and is especially relevant for analyzing human motions. We consider trajectories acquired from sign language, and show the interest of considering curvature and also torsion in their analysis, both being physically meaningful.

---

## 11. Flexible unimodal density estimation in hidden Markov models

**作者**: Jan-Ole Koslik, Fanny Dupont, Marie Auger-Méthé, Marianne Marcoux, Nancy Heckman

**PDF链接**: [https://arxiv.org/pdf/2511.17071.pdf](https://arxiv.org/pdf/2511.17071.pdf)

**摘要**: 1. Hidden Markov models (HMMs) are powerful tools for modelling time-series data with underlying state structure. However, selecting appropriate parametric forms for the state-dependent distributions is often challenging and can lead to model misspecification. To address this, P-spline-based nonparametric estimation of state-dependent densities has been proposed. While offering great flexibility, these approaches can result in overly complex densities (e.g. bimodal) that hinder interpretability. 2. We propose a straightforward method that builds on shape-constrained spline theory to enforce unimodality in the estimated state-dependent densities through enforcing unimodality of the spline coefficients. This constraint strikes a practical balance between model flexibility, interpretability, and parsimony. 3. Through two simulation studies and a real-world case study using narwhal (Monodon monoceros) dive data, we demonstrate the proposed approach yields more stable estimates compared to fully flexible, unconstrained models improving model performance and interpretability. 4. Our method bridges a key methodological gap, by providing a parsimonious HMM framework that balances the interpretability of parametric models with the flexibility of nonparametric estimation. This provides ecologists with a powerful tool to derive ecologically meaningful inference from telemetry data while avoiding the pitfalls of overly complex models.

---

## 12. ggskewboxplots: Enhanced Boxplots for Skewed Data in R

**作者**: Mustafa Cavus

**PDF链接**: [https://arxiv.org/pdf/2511.17091.pdf](https://arxiv.org/pdf/2511.17091.pdf)

**摘要**: Traditional boxplots are widely used for summarizing and visualizing the distribution of numerical data, yet they exhibit significant limitations when applied to skewed or heavy-tailed distributions, often leading to misclassification of outliers through swamping -- flagging typical observations as outliers -- or masking -- failing to detect true outliers. This paper addresses these limitations by systematically evaluating several alternative boxplots specifically designed to accommodate distributional asymmetry. We introduce ggskewboxplots, an R package that integrates multiple robust and skewness-aware boxplot variants, providing a unified and user-friendly framework for exploratory data analysis. Using extensive Monte Carlo simulations under controlled skewness and kurtosis conditions, implemented via the mosaic approach based on the Skewed Exponential Power distribution, we assess the sensitivity and specificity of each method. Simulation results indicate that classical Tukey-style boxplots are highly prone to swamping and masking, whereas robust skewness-adjusted variants -- particularly those leveraging quartile-based skewness measures or medcouple-based adjustments -- achieve substantially better performance. These findings offer practical guidance for selecting reliable boxplot methods in applied settings and demonstrate how the ggskewboxplots package facilitates accessible, distribution-aware visualizations within the familiar ggplot2 workflow.

---

## 13. Modified Delayed Acceptance MCMC for Quasi-Bayesian Inference with Linear Moment Conditions

**作者**: Masahiro Tanaka

**PDF链接**: [https://arxiv.org/pdf/2511.17117.pdf](https://arxiv.org/pdf/2511.17117.pdf)

**摘要**: We develop a computationally efficient framework for quasi-Bayesian inference based on linear moment conditions. The approach employs a delayed acceptance Markov chain Monte Carlo (DA-MCMC) algorithm that uses a surrogate target kernel and a proposal distribution derived from an approximate conditional posterior, thereby exploiting the structure of the quasi-likelihood. Two implementations are introduced. DA-MCMC-Exact fully incorporates prior information into the proposal distribution and maximizes per-iteration efficiency, whereas DA-MCMC-Approx omits the prior in the proposal to reduce matrix inversions, improving numerical stability and computational speed in higher dimensions. Simulation studies on heteroskedastic linear regressions show substantial gains over standard MCMC and conventional DA-MCMC baselines, measured by multivariate effective sample size per iteration and per second. The Approx variant yields the best overall throughput, while the Exact variant attains the highest per-iteration efficiency. Applications to two empirical instrumental variable regressions corroborate these findings: the Approx implementation scales to larger designs where other methods become impractical, while still delivering precise inference. Although developed for moment-based quasi-posteriors, the proposed approach also extends to risk-based quasi-Bayesian formulations when first-order conditions are linear and can be transformed analogously. Overall, the proposed algorithms provide a practical and robust tool for quasi-Bayesian analysis in statistical applications.

---

## 14. A spatiotemporal Bayesian hierarchical model of heat-related mortality in Catalonia, Spain (2012--2022): The role of environmental and socioeconomic modifiers

**作者**: David Solano, Marta Solans, Xavier Perafita, Anna Ruiz-Comellas, Marc Saez, Maria A. Barceló

**PDF链接**: [https://arxiv.org/pdf/2511.17148.pdf](https://arxiv.org/pdf/2511.17148.pdf)

**摘要**: Background: Extreme heat is a major public health risk, yet its relationship with mortality may be confounded or modified by air pollution and social determinants. Objectives: We aimed to quantify the effects of extreme maximum temperatures and heatwaves on daily mortality in Catalonia (2012--2022), and to assess the modifying and confounding roles of air pollutants and socioeconomic factors. Methods: We conducted a time--series ecological study across 379 basic health areas (ABS) during summer months. Mortality data from the Spanish National Statistics Institute were linked with meteorological and air pollution data. A hierarchical Bayesian spatiotemporal model, incorporating structured and unstructured random effects, was used to account for spatial and temporal dependencies, as well as observed socioeconomic confounders. Results: In total, 730,634 deaths occurred, with 216,989 in summer. Extreme heat alone was not independently associated with mortality, as its effect was fully confounded by high ozone levels and partly by socioeconomic indicators. Ozone concentrations ($\ge 120 \mu g/m^3$) significantly increased mortality risk, especially among individuals aged $\ge 85$ years. Greater income inequality and higher proportions of older residents also amplified vulnerability. Conclusion: Mortality risks from extreme heat in Catalonia were strongly influenced by ozone levels and social determinants. Adaptation strategies should address both compound environmental exposures together with socioeconomic vulnerability to better protect older and disadvantaged populations.

---

## 15. Differentially private testing for relevant dependencies in high dimensions

**作者**: Patrick Bastian, Holger Dette, Martin Dunsche

**PDF链接**: [https://arxiv.org/pdf/2511.17167.pdf](https://arxiv.org/pdf/2511.17167.pdf)

**摘要**: We investigate the problem of detecting dependencies between the components of a high-dimensional vector. Our approach advances the existing literature in two important respects. First, we consider the problem under privacy constraints. Second, instead of testing whether the coordinates are pairwise independent, we are interested in determining whether certain pairwise associations between the components (such as all pairwise Kendall's $\tau$ coefficients) do not exceed a given threshold in absolute value. Considering hypotheses of this form is motivated by the observation that in the high-dimensional regime, it is rare and perhaps impossible to have a null hypothesis that can be modeled exactly by assuming that all pairwise associations are precisely equal to zero.
The formulation of the null hypothesis as a composite hypothesis makes the problem of constructing tests already non-standard in the non-private setting. Additionally, under privacy constraints, state of the art procedures rely on permutation approaches that are rendered invalid under a composite null. We propose a novel bootstrap based methodology that is especially powerful in sparse settings, develop theoretical guarantees under mild assumptions and show that the proposed method enjoys good finite sample properties even in the high privacy regime. Additionally, we present applications in medical data that showcase the applicability of our methodology.

---

## 16. Nonparametric Inference for Extreme CoVaR and CoES

**作者**: Qingzhao Zhong, Yanxi Hou

**PDF链接**: [https://arxiv.org/pdf/2511.17180.pdf](https://arxiv.org/pdf/2511.17180.pdf)

**摘要**: Systemic risk measures quantify the potential risk to an individual financial constituent arising from the distress of entire financial system. As a generalization of two widely applied risk measures, Value-at-Risk and Expected Shortfall, the Conditional Value-at-Risk (CoVaR) and Conditional Expected Shortfall (CoES) have recently been receiving growing attention on applications in economics and finance, since they serve as crucial metrics for systemic risk measurement. However, existing approaches confront some challenges in statistical inference and asymptotic theories when estimating CoES, particularly at high risk levels. In this paper, within a framework of upper tail dependence, we propose several extrapolative methods to estimate both extreme CoVaR and CoES nonparametrically via an adjustment factor, which are intimately related to the nonparametric modelling of the tail dependence function. In addition, we study the asymptotic theories of all proposed extrapolative methods based on multivariate extreme value theory. Finally, some simulations and real data analyses are conducted to demonstrate the empirical performances of our methods.

---

## 17. Properties of stepwise parameter estimation in high-dimensional vine copulas

**作者**: Jana Gauss, Thomas Nagler

**PDF链接**: [https://arxiv.org/pdf/2511.17291.pdf](https://arxiv.org/pdf/2511.17291.pdf)

**摘要**: The increasing use of vine copulas in high-dimensional settings, where the number of parameters is often of the same order as the sample size, calls for asymptotic theory beyond the traditional fixed-$p$, large-$n$ framework. We establish consistency and asymptotic normality of the stepwise maximum likelihood estimator for vine copulas when the number of parameters diverges as $n \to \infty$. Our theoretical results cover both parametric and nonparametric estimation of the marginal distributions, as well as truncated vines, and are also applicable to general estimation problems, particularly other sequential procedures. Numerical experiments suggest that the derived assumptions are satisfied if the pair copulas in higher trees converge to independence copulas sufficiently fast. A simulation study substantiates these findings and identifies settings in which estimation becomes challenging. In particular, the vine structure strongly affects estimation accuracy, with D-vines being more difficult to estimate than C-vines, and estimates in Gumbel vines exhibit substantially larger biases than those in Gaussian vines.

---

## 18. The Experimental Unit Information Index: Balancing Evidentiary Value and Sample Size of Adaptive Designs

**作者**: Leonhard Held, Fadoua Balabdaoui, Samuel Pawel

**PDF链接**: [https://arxiv.org/pdf/2511.17292.pdf](https://arxiv.org/pdf/2511.17292.pdf)

**摘要**: Reducing the number of experimental units is one of the three pillars of the 3R principles (Replace, Reduce, Refine) in animal research. At the same time, statistical error rates need to be controlled to enable reliable inferences and decisions. This paper proposes a novel measure to quantify the evidentiary value of one experimental unit for a given study design. The experimental unit information index (EUII) is based on power, Type-I error and sample size, and has attractive interpretations both in terms of frequentist error rates and Bayesian posterior odds. We introduce the EUII in simple statistical test settings and show that its asymptotic value depends only on the assumed relative effect size under the alternative. We then extend the definition to adaptive designs where early stopping for efficacy or futility may cause reductions in sample size. Applications to group-sequential designs and a recently proposed adaptive statistical test procedure show the usefulness of the approach when the goal is to maximize the evidentiary value of one experimental unit.

---

## 19. Covariate Connectivity Combined Clustering for Weighted Networks

**作者**: Zeyu Hu, Wenrui Li, Jun Yan, Panpan Zhang

**PDF链接**: [https://arxiv.org/pdf/2511.17302.pdf](https://arxiv.org/pdf/2511.17302.pdf)

**摘要**: Community detection is a central task in network analysis, with applications in social, biological, and technological systems. Traditional algorithms rely primarily on network topology, which can fail when community signals are partly encoded in node-specific attributes. Existing covariate-assisted methods often assume the number of clusters is known, involve computationally intensive inference, or are not designed for weighted networks. We propose $\text{C}^4$: Covariate Connectivity Combined Clustering, an adaptive spectral clustering algorithm that integrates network connectivity and node-level covariates into a unified similarity representation. $\text{C}^4$ balances the two sources of information through a data-driven tuning parameter, estimates the number of communities via an eigengap heuristic, and avoids reliance on costly sampling-based procedures. Simulation studies show that $\text{C}^4$ achieves higher accuracy and robustness than competing approaches across diverse scenarios. Application to an airport reachability network demonstrates the method's scalability, interpretability, and practical utility for real-world weighted networks.

---

## 20. U-DESPE: a Bayesian Utility-based methodology for dosing regimen optimization in early-phase oncology trials based on Dose-Exposure, Safety, Pharmacodynamics, Efficacy

**作者**: Anaïs Andrillon, Sandrine Micallef, Moreno Ursino, Pavel Mozgunov, Marie-Karelle Riviere

**PDF链接**: [https://arxiv.org/pdf/2511.17376.pdf](https://arxiv.org/pdf/2511.17376.pdf)

**摘要**: With the development of novel therapies such as molecularly targeted agents and immunotherapy, the maximum tolerated dose paradigm that "more is better" does not necessarily hold anymore. In this context, doses and schedules of novel therapies may be inadequately characterized and oncology drug dose-finding approaches should be revised. This is increasingly recognized by health authorities, notably through the Optimus project. We developed a Bayesian dose-finding design, called U-DESPE, which allows to either determine the optimal dosing regimen at the end of the dose-escalation phase, or use of dedicated cohorts for randomizing patients to candidate optimal dosing regimens after that safe dosing regimens have been found. U-DESPE design relies on a dose-exposure model built from pharmacokinetic data using non-linear mixed-effect modeling approaches. Then three models are built to assess the relationships between exposure and the probability of selected relevant endpoints on safety, efficacy, and pharmacodynamics. These models are then combined to predict the different endpoints for every candidate dosing regimens. Finally, a utility function is proposed to quantify the trade-off between these endpoints and to determine the optimal dosing regimen. We applied the proposed method on a clinical trial case study and performed an extensive simulation study to evaluate the operating characteristics of the method.

---

## 21. On treating right-censoring events like treatments

**作者**: Lan Wen, Aaron L. Sarvet, Jessica G. Young

**PDF链接**: [https://arxiv.org/pdf/2511.17379.pdf](https://arxiv.org/pdf/2511.17379.pdf)

**摘要**: In causal inference literature, potential outcomes are often indexed by the "elimination of all right-censoring events," leading to the perception that such a restriction is necessary for defining well-posed causal estimands. In this paper, we clarify that this restriction is not required: a well-defined estimand can be formulated without indexing on the elimination of such events. Achieving this requires a more precise classification of right-censoring events than has historically been considered, as the nature of these events has direct implications for identification of the target estimand. We provide a framework that distinguishes different types of right-censoring events from a causal perspective, and demonstrate how this framework relates to censoring definitions and assumptions in classical survival analysis literature. By bridging these perspectives, we provide a clearer understanding of how to handle right-censoring events and provide guidance for identifying causal estimands when right-censored events are present.

---

## 22. Background on real and complex elliptically symmetric distributions

**作者**: Jean-Pierre Delmas

**PDF链接**: [https://arxiv.org/pdf/2511.17394.pdf](https://arxiv.org/pdf/2511.17394.pdf)

**摘要**: This chapter presents a short overview of real elliptically symmetric (RES) distributions, complemented by circular complex elliptically symmetric (C-CES) and noncircular CES (NC-CES) distributions as complex representations of RES distributions. These distributions are both an extension of the multivariate Gaussian distribution and a multivariate extension of univariate symmetric distributions. They are equivalently defined through their characteristic functions and their stochastic representations, which naturally follow from the spherically symmetric distributions after affine transformations. Particular attention is paid to the absolutely continuous case and to the subclass of compound Gaussian distributions. Results related to moments, affine transformations, marginal and conditional distributions, and summation stability are also presented. Some well-known instances of RES distributions are provided with their main properties. Finally, the estimation of the symmetry center and scatter matrix is briefly discussed through the sample mean (SM), sample covariance matrix (SCM) estimate, maximum estimate (ML), $M$-estimators, and Tyler's $M$-estimators. Particular attention will be paid to the asymptotic Gaussianity of the $M$-estimators of the scatter matrix. To conclude, some hints about the Slepian-Bangs formula are provided.

---

## 23. Bayesian Bridge Gaussian Process Regression

**作者**: Minshen Xu, Shiwei Lan, Lulu Kang

**PDF链接**: [https://arxiv.org/pdf/2511.17415.pdf](https://arxiv.org/pdf/2511.17415.pdf)

**摘要**: The performance of Gaussian Process (GP) regression is often hampered by the curse of dimensionality, which inflates computational cost and reduces predictive power in high-dimensional problems. Variable selection is thus crucial for building efficient and accurate GP models. Inspired by Bayesian bridge regression, we propose the Bayesian Bridge Gaussian Process Regression (B\textsuperscript{2}GPR) model. This framework places $\ell_q$-norm constraints on key GP parameters to automatically induce sparsity and identify active variables. We formulate two distinct versions: one for $q=2$ using conjugate Gaussian priors, and another for $0<q<2$ that employs constrained flat priors, leading to non-standard, norm-constrained posterior distributions. To enable posterior inference, we design a Gibbs sampling algorithm that integrates Spherical Hamiltonian Monte Carlo (SphHMC) to efficiently sample from the constrained posteriors when $0<q<2$. Simulations and a real-data application confirm that B\textsuperscript{2}GPR offers superior variable selection and prediction compared to alternative approaches.

---

## 24. Iterating marginalized Bayes maps for likelihood maximization with application to nonlinear panel models

**作者**: Jesse Wheeler, Aaron J. Abkemeier, Edward L. Ionides

**PDF链接**: [https://arxiv.org/pdf/2511.17438.pdf](https://arxiv.org/pdf/2511.17438.pdf)

**摘要**: Complex dynamic systems can be investigated by fitting mechanistic stochastic dynamic models to time series data. In this context, commonly used Monte Carlo inference procedures for model selection and parameter estimation quickly become computationally unfeasible as the system dimension grows. The increasing prevalence of panel data, characterized by multiple related time series, therefore necessitates the development of inference algorithms that are effective for this class of high-dimensional mechanistic models. Nonlinear, non-Gaussian mechanistic models are routinely fitted to time series data but seldom to panel data, despite its widespread availability, suggesting that the practical difficulties for existing procedures are prohibitive. We investigate the use of iterated filtering algorithms for this purpose. We introduce a novel algorithm that contains a marginalization step that mitigates issues arising from particle filtering in high dimensions. Our approach enables likelihood-based inference for models that were previously considered intractable, thus broadening the scope of dynamic models available for panel data analysis.

---

## 25. Extending the Accelerated Failure Conditionals Model to Location-Scale Families

**作者**: Jared N. Lakhani

**PDF链接**: [https://arxiv.org/pdf/2511.17463.pdf](https://arxiv.org/pdf/2511.17463.pdf)

**摘要**: Arnold and Arvanitis (2020) introduced a novel class of bivariate conditionally specified distributions, in which dependence between two random variables is established by defining the distribution of one variable conditional on the other. This conditioning regime was formulated through survival functions and termed the accelerated failure conditionals model. Subsequently, Lakhani (2025) extended this conditioning framework to encompass distributional families whose marginal densities may exhibit unimodality and skewness, thereby moving beyond families with non-increasing densities. The present study builds on this line of work by proposing a conditional survival specification derived from a location-scale distributional family, where the dependence between $X$ and $Y$ arises not only through the acceleration function but also via a location function. An illustrative example of this new specification is developed using a Weibull marginal for $X$. The resulting models are fully characterized by closed-form expressions for their moments, and simulations are implemented using the Metropolis-Hastings algorithm. Finally, the model is applied to a dataset in which the empirical distribution of $Y$ lies on the real line, demonstrating the models' capacity to accommodate $Y$ marginals defined over $\mathbb{R}$.

---

## 26. Efficient Penalty-Based Bilevel Methods: Improved Analysis, Novel Updates, and Flatness Condition

**作者**: Liuyuan Jiang, Quan Xiao, Lisha Chen, Tianyi Chen

**PDF链接**: [https://arxiv.org/pdf/2511.16796.pdf](https://arxiv.org/pdf/2511.16796.pdf)

**摘要**: Penalty-based methods have become popular for solving bilevel optimization (BLO) problems, thanks to their effective first-order nature. However, they often require inner-loop iterations to solve the lower-level (LL) problem and small outer-loop step sizes to handle the increased smoothness induced by large penalty terms, leading to suboptimal complexity. This work considers the general BLO problems with coupled constraints (CCs) and leverages a novel penalty reformulation that decouples the upper- and lower-level variables. This yields an improved analysis of the smoothness constant, enabling larger step sizes and reduced iteration complexity for Penalty-Based Gradient Descent algorithms in ALTernating fashion (ALT-PBGD). Building on the insight of reduced smoothness, we propose PBGD-Free, a novel fully single-loop algorithm that avoids inner loops for the uncoupled constraint BLO. For BLO with CCs, PBGD-Free employs an efficient inner-loop with substantially reduced iteration complexity. Furthermore, we propose a novel curvature condition describing the "flatness" of the upper-level objective with respect to the LL variable. This condition relaxes the traditional upper-level Lipschitz requirement, enables smaller penalty constant choices, and results in a negligible penalty gradient term during upper-level variable updates. We provide rigorous convergence analysis and validate the method's efficacy through hyperparameter optimization for support vector machines and fine-tuning of large language models.

---

## 27. Approximate Least-Favorable Distributions and Nearly Optimal Tests via Stochastic Mirror Descent

**作者**: Andrés Aradillas Fernández, José Blanchet, José Luis Montiel Olea, Chen Qiu, Jörg Stoye, Lezhi Tan

**PDF链接**: [https://arxiv.org/pdf/2511.16925.pdf](https://arxiv.org/pdf/2511.16925.pdf)

**摘要**: We consider a class of hypothesis testing problems where the null hypothesis postulates $M$ distributions for the observed data, and there is only one possible distribution under the alternative. We show that one can use a stochastic mirror descent routine for convex optimization to provably obtain - after finitely many iterations - both an approximate least-favorable distribution and a nearly optimal test, in a sense we make precise. Our theoretical results yield concrete recommendations about the algorithm's implementation, including its initial condition, its step size, and the number of iterations. Importantly, our suggested algorithm can be viewed as a slight variation of the algorithm suggested by Elliott, Müller, and Watson (2015), whose theoretical performance guarantees are unknown.

---

## 28. Diffusion-Inversion-Net (DIN): An End-to-End Direct Probabilistic Framework for Characterizing Hydraulic Conductivities and Quantifying Uncertainty

**作者**: Xun Zhang, Weijie Yang, Jiangjiang Zhang, Simin Jiang

**PDF链接**: [https://arxiv.org/pdf/2511.16926.pdf](https://arxiv.org/pdf/2511.16926.pdf)

**摘要**: We propose the Diffusion-Inversion-Net (DIN) framework for inverse modeling of groundwater flow and solute transport processes. DIN utilizes an offline-trained Denoising Diffusion Probabilistic Model (DDPM) as a powerful prior leaner, which flexibly incorporates sparse, multi-source observational data, including hydraulic head, solute concentration, and hard conductivity data, through conditional injection mechanisms. These conditioning inputs subsequently guide the generative inversion process during sampling. Bypassing iterative forward simulations, DIN leverages stochastic sampling and probabilistic modeling mechanisms to directly generate ensembles of posterior parameter fields by repeatedly executing the reverse denoising process. Two representative posterior scenarios, Gaussian and non-Gaussian, are investigated. The results demonstrate that DIN can produce multiple constraint-satisfying realizations under identical observational conditions, accurately estimate hydraulic-conductivity fields, and achieve reliable uncertainty quantification. The framework exhibits strong generalization capability across diverse data distributions, offering a robust and unified alternative to conventional multi-stage inversion methodologies.

---

## 29. Gradient flow for deep equilibrium single-index models

**作者**: Sanjit Dandapanthula, Aaditya Ramdas

**PDF链接**: [https://arxiv.org/pdf/2511.16976.pdf](https://arxiv.org/pdf/2511.16976.pdf)

**摘要**: Deep equilibrium models (DEQs) have recently emerged as a powerful paradigm for training infinitely deep weight-tied neural networks that achieve state of the art performance across many modern machine learning tasks. Despite their practical success, theoretically understanding the gradient descent dynamics for training DEQs remains an area of active research. In this work, we rigorously study the gradient descent dynamics for DEQs in the simple setting of linear models and single-index models, filling several gaps in the literature. We prove a conservation law for linear DEQs which implies that the parameters remain trapped on spheres during training and use this property to show that gradient flow remains well-conditioned for all time. We then prove linear convergence of gradient descent to a global minimizer for linear DEQs and deep equilibrium single-index models under appropriate initialization and with a sufficiently small step size. Finally, we validate our theoretical findings through experiments.

---

## 30. DAPS++: Rethinking Diffusion Inverse Problems with Decoupled Posterior Annealing

**作者**: Hao Chen, Renzheng Zhang, Scott S. Howard

**PDF链接**: [https://arxiv.org/pdf/2511.17038.pdf](https://arxiv.org/pdf/2511.17038.pdf)

**摘要**: From a Bayesian perspective, score-based diffusion solves inverse problems through joint inference, embedding the likelihood with the prior to guide the sampling process. However, this formulation fails to explain its practical behavior: the prior offers limited guidance, while reconstruction is largely driven by the measurement-consistency term, leading to an inference process that is effectively decoupled from the diffusion dynamics. To clarify this structure, we reinterpret the role of diffusion in inverse problem solving as an initialization stage within an expectation--maximization (EM)--style framework, where the diffusion stage and the data-driven refinement are fully decoupled. We introduce \textbf{DAPS++}, which allows the likelihood term to guide inference more directly while maintaining numerical stability and providing insight into why unified diffusion trajectories remain effective in practice. By requiring fewer function evaluations (NFEs) and measurement-optimization steps, \textbf{DAPS++} achieves high computational efficiency and robust reconstruction performance across diverse image restoration tasks.

---

## 31. Automobile demand forecasting: Spatiotemporal and hierarchical modeling, life cycle dynamics, and user-generated online information

**作者**: Tom Nahrendorf, Stefan Minner, Helfried Binder, Richard Zinck

**PDF链接**: [https://arxiv.org/pdf/2511.17275.pdf](https://arxiv.org/pdf/2511.17275.pdf)

**摘要**: Premium automotive manufacturers face increasingly complex forecasting challenges due to high product variety, sparse variant-level data, and volatile market dynamics. This study addresses monthly automobile demand forecasting across a multi-product, multi-market, and multi-level hierarchy using data from a German premium manufacturer. The methodology combines point and probabilistic forecasts across strategic and operational planning levels, leveraging ensembles of LightGBM models with pooled training sets, quantile regression, and a mixed-integer linear programming reconciliation approach. Results highlight that spatiotemporal dependencies, as well as rounding bias, significantly affect forecast accuracy, underscoring the importance of integer forecasts for operational feasibility. Shapley analysis shows that short-term demand is reactive, shaped by life cycle maturity, autoregressive momentum, and operational signals, whereas medium-term demand reflects anticipatory drivers such as online engagement, planning targets, and competitive indicators, with online behavioral data considerably improving accuracy at disaggregated levels.

---

## 32. SAVeD: Semantic Aware Version Discovery

**作者**: Artem Frenk, Roee Shraga

**PDF链接**: [https://arxiv.org/pdf/2511.17298.pdf](https://arxiv.org/pdf/2511.17298.pdf)

**摘要**: Our work introduces SAVeD (Semantically Aware Version Detection), a contrastive learning-based framework for identifying versions of structured datasets without relying on metadata, labels, or integration-based assumptions. SAVeD addresses a common challenge in data science of repeated labor due to a difficulty of similar work or transformations on datasets. SAVeD employs a modified SimCLR pipeline, generating augmented table views through random transformations (e.g., row deletion, encoding perturbations). These views are embedded via a custom transformer encoder and contrasted in latent space to optimize semantic similarity. Our model learns to minimize distances between augmented views of the same dataset and maximize those between unrelated tables. We evaluate performance using validation accuracy and separation, defined respectively as the proportion of correctly classified version/non-version pairs on a hold-out set, and the difference between average similarities of versioned and non-versioned tables (defined by a benchmark, and not provided to the model). Our experiments span five canonical datasets from the Semantic Versioning in Databases Benchmark, and demonstrate substantial gains post-training. SAVeD achieves significantly higher accuracy on completely unseen tables in, and a significant boost in separation scores, confirming its capability to distinguish semantically altered versions. Compared to untrained baselines and prior state-of-the-art dataset-discovery methods like Starmie, our custom encoder achieves competitive or superior results.

---

## 33. Is Phase Really Needed for Weakly-Supervised Dereverberation ?

**作者**: Marius Rodrigues, Louis Bahrman, Roland Badeau, Gaël Richard

**PDF链接**: [https://arxiv.org/pdf/2511.17346.pdf](https://arxiv.org/pdf/2511.17346.pdf)

**摘要**: In unsupervised or weakly-supervised approaches for speech dereverberation, the target clean (dry) signals are considered to be unknown during training. In that context, evaluating to what extent information can be retrieved from the sole knowledge of reverberant (wet) speech becomes critical. This work investigates the role of the reverberant (wet) phase in the time-frequency domain. Based on Statistical Wave Field Theory, we show that late reverberation perturbs phase components with white, uniformly distributed noise, except at low frequencies. Consequently, the wet phase carries limited useful information and is not essential for weakly supervised dereverberation. To validate this finding, we train dereverberation models under a recent weak supervision framework and demonstrate that performance can be significantly improved by excluding the reverberant phase from the loss function.

---

## 34. Self-Supervised Learning by Curvature Alignment

**作者**: Benyamin Ghojogh, M.Hadi Sepanj, Paul Fieguth

**PDF链接**: [https://arxiv.org/pdf/2511.17426.pdf](https://arxiv.org/pdf/2511.17426.pdf)

**摘要**: Self-supervised learning (SSL) has recently advanced through non-contrastive methods that couple an invariance term with variance, covariance, or redundancy-reduction penalties. While such objectives shape first- and second-order statistics of the representation, they largely ignore the local geometry of the underlying data manifold. In this paper, we introduce CurvSSL, a curvature-regularized self-supervised learning framework, and its RKHS extension, kernel CurvSSL. Our approach retains a standard two-view encoder-projector architecture with a Barlow Twins-style redundancy-reduction loss on projected features, but augments it with a curvature-based regularizer. Each embedding is treated as a vertex whose $k$ nearest neighbors define a discrete curvature score via cosine interactions on the unit hypersphere; in the kernel variant, curvature is computed from a normalized local Gram matrix in an RKHS. These scores are aligned and decorrelated across augmentations by a Barlow-style loss on a curvature-derived matrix, encouraging both view invariance and consistency of local manifold bending. Experiments on MNIST and CIFAR-10 datasets with a ResNet-18 backbone show that curvature-regularized SSL yields competitive or improved linear evaluation performance compared to Barlow Twins and VICReg. Our results indicate that explicitly shaping local geometry is a simple and effective complement to purely statistical SSL regularizers.

---

## 35. Minimax Statistical Estimation under Wasserstein Contamination

**作者**: Patrick Chao, Edgar Dobriban

**PDF链接**: [https://arxiv.org/pdf/2308.01853.pdf](https://arxiv.org/pdf/2308.01853.pdf)

**摘要**: Contaminations are a key concern in modern statistical learning, as small but systematic perturbations of all datapoints can substantially alter estimation results. Here, we study Wasserstein-$r$ contaminations ($r\ge 1$) in an $\ell_q$ norm ($q\in [1,\infty]$), in which each observation may undergo an adversarial perturbation with bounded cost, complementing the classical Huber model, corresponding to total variation norm, where only a fraction of observations is arbitrarily corrupted. We study both independent and joint (coordinated) contaminations and develop a minimax theory under $\ell_q^r$ losses.
Our analysis encompasses several fundamental problems: location estimation, linear regression, and pointwise nonparametric density estimation. For joint contaminations in location estimation and for prediction in linear regression, we obtain the exact minimax risk, identify least favorable contaminations, and show that the sample mean and least squares predictor are respectively minimax optimal. For location estimation under independent contaminations, we give sharp upper and lower bounds, including exact minimaxity in the Euclidean Wasserstein contamination case, when $q=r=2$. For pointwise density estimation in any dimension, we derive the optimal rate, showing that it is achieved by kernel density estimation with a bandwidth that is possibly larger than the classical one.
Our proofs leverage powerful tools from optimal transport developed over the last 20 years, including the dynamic Benamou-Brenier formulation. Taken together, our results suggest that in contrast to the Huber contamination model, for norm-based Wasserstein contaminations, classical estimators may be nearly optimally robust.

---

## 36. The $\ell$-test: leveraging sparsity in the Gaussian linear model for improved inference

**作者**: Souhardya Sengupta, Lucas Janson

**PDF链接**: [https://arxiv.org/pdf/2406.18390.pdf](https://arxiv.org/pdf/2406.18390.pdf)

**摘要**: We develop novel LASSO-based methods for coefficient testing and confidence interval construction in the Gaussian linear model with $n\ge d$. Our methods' finite-sample validity is identical to that of their ubiquitous ordinary-least-squares-$t$-test-based analogues, yet have substantially higher power when the true coefficient vector is sparse. In particular, under sparsity our coefficient test, which we call the $\ell$-test, performs like the \emph{one-sided} $t$-test (despite not being given any information about the sign), and $\ell$-test-based confidence intervals are correspondingly shorter than the standard $t$-test-based intervals. The nature of the $\ell$-test directly provides a novel exact adjustment conditional on LASSO selection for post-selection inference, allowing for the construction of post-selection $p$-values and confidence intervals. None of our methods require resampling or Monte Carlo estimation. We perform a variety of simulations and a real data analysis on an HIV drug resistance data set to demonstrate the benefits of the $\ell$-test. We additionally show that the $\ell$-test can be applied to a large class of asymptotically Gaussian estimators, dramatically expanding its applicability beyond linear models.

---

## 37. (De)-regularized Maximum Mean Discrepancy Gradient Flow

**作者**: Zonghao Chen, Aratrika Mustafi, Pierre Glaser, Anna Korba, Arthur Gretton, Bharath K. Sriperumbudur

**PDF链接**: [https://arxiv.org/pdf/2409.14980.pdf](https://arxiv.org/pdf/2409.14980.pdf)

**摘要**: We introduce a (de)-regularization of the Maximum Mean Discrepancy (DrMMD) and its Wasserstein gradient flow. Existing gradient flows that transport samples from source distribution to target distribution with only target samples, either lack tractable numerical implementation ($f$-divergence flows) or require strong assumptions, and modifications such as noise injection, to ensure convergence (Maximum Mean Discrepancy flows). In contrast, DrMMD flow can simultaneously (i) guarantee near-global convergence for a broad class of targets in both continuous and discrete time, and (ii) be implemented in closed form using only samples. The former is achieved by leveraging the connection between the DrMMD and the $\chi^2$-divergence, while the latter comes by treating DrMMD as MMD with a de-regularized kernel. Our numerical scheme uses an adaptive de-regularization schedule throughout the flow to optimally trade off between discretization errors and deviations from the $\chi^2$ regime. The potential application of the DrMMD flow is demonstrated across several numerical experiments, including a large-scale setting of training student/teacher networks.

---

## 38. The construction of augmented designs in square arrays

**作者**: E. R. Williams, H-P. Piepho

**PDF链接**: [https://arxiv.org/pdf/2501.08448.pdf](https://arxiv.org/pdf/2501.08448.pdf)

**摘要**: Augmented designs are typically used in early-stage breeding programs to compare single replicates of test entries by combining them with replicated check varieties. One or two dimensional incomplete blocking can be incorporated in the design to accommodate possible site variation. An augmented design in a square array can be derived from a smaller row-column design (the contraction). In a recent paper Bailey and Haines (2025) investigated the link between an augmented design in a square array and its contraction. Here we formally establish this connection by expressing the average efficiency factor of the augmented design in terms of that of its contraction. A consequence of this is that an optimal contraction can be used to construct an optimal augmented design. The table of cyclic contractions presented by Bailey and Haines (2025) is updated in terms of optimality. Specifically, in cases where a cyclic contraction is not optimal, an augmented design with optimal or near-optimal efficiency can be obtained via computer search.

---

## 39. Conditional Extreme Value Estimation for Dependent Time Series

**作者**: Martin Bladt, Laurits Glargaard, Theodor Henningsen

**PDF链接**: [https://arxiv.org/pdf/2503.22366.pdf](https://arxiv.org/pdf/2503.22366.pdf)

**摘要**: We study the consistency and weak convergence of the conditional tail function and conditional Hill estimators under broad dependence assumptions for a heavy-tailed response sequence and a covariate sequence. Consistency is established under $\alpha$-mixing, while asymptotic normality follows from $\beta$-mixing and second-order conditions. A key aspect of our approach is its versatile functional formulation in terms of the conditional tail process. Simulations demonstrate its performance across dependence scenarios. We apply our method to extreme event modelling in the oil industry, revealing distinct tail behaviours under varying conditioning values.

---

## 40. Extension of Dynamic Network Biomarker using the propensity score method: Simulation of causal effects on variance and correlation coefficient

**作者**: Satoru Shinoda, Hideaki Kawaguchi

**PDF链接**: [https://arxiv.org/pdf/2505.13846.pdf](https://arxiv.org/pdf/2505.13846.pdf)

**摘要**: In clinical biomarker studies, the Dynamic Network Biomarker (DNB) is sometimes used. DNB is a composite variable derived from the variance and the Pearson correlation coefficient of biological signals. When applying DNB to clinical data, it is important to account for confounding bias. However, little attention has been paid to statistical causal inference methods for variance and correlation coefficients. This study evaluates confounding adjustment using propensity score matching (PSM) through Monte Carlo simulations. Our results support the use of PSM to reduce bias and improve group comparisons when DNB is applied to clinical data.

---

## 41. Optimal Convergence Rates of Deep Neural Network Classifiers

**作者**: Zihan Zhang, Lei Shi, Ding-Xuan Zhou

**PDF链接**: [https://arxiv.org/pdf/2506.14899.pdf](https://arxiv.org/pdf/2506.14899.pdf)

**摘要**: In this paper, we study the binary classification problem on $[0,1]^d$ under the Tsybakov noise condition (with exponent $s \in [0,\infty]$) and the compositional assumption. This assumption requires the conditional class probability function of the data distribution to be the composition of $q+1$ vector-valued multivariate functions, where each component function is either a maximum value function or a Hölder-$\beta$ smooth function that depends only on $d_*$ of its input variables. Notably, $d_*$ can be significantly smaller than the input dimension $d$. We prove that, under these conditions, the optimal convergence rate for the excess 0-1 risk of classifiers is $\left( \frac{1}{n} \right)^{\frac{\beta\cdot(1\wedge\beta)^q}{{\frac{d_*}{s+1}+(1+\frac{1}{s+1})\cdot\beta\cdot(1\wedge\beta)^q}}}$, which is independent of the input dimension $d$. Additionally, we demonstrate that ReLU deep neural networks (DNNs) trained with hinge loss can achieve this optimal convergence rate up to a logarithmic factor. This result provides theoretical justification for the excellent performance of ReLU DNNs in practical classification tasks, particularly in high-dimensional settings. The generalized approach is of independent interest.

---

## 42. Auto-Doubly Robust Estimation of Causal Effects on a Network

**作者**: Jizhou Liu, Dake Zhang, Eric J. Tchetgen Tchetgen

**PDF链接**: [https://arxiv.org/pdf/2506.23332.pdf](https://arxiv.org/pdf/2506.23332.pdf)

**摘要**: This paper develops new methods for causal inference in observational studies on a single large network of interconnected units, addressing two key challenges: long-range dependence among units and the presence of general interference. We introduce a novel network version of Augmented Inverse Propensity Weighted, which combines propensity score and outcome models defined on the network to achieve doubly robust identification and estimation of both direct and spillover causal effects. Under a network version of conditional ignorability, the proposed approach identifies the expected potential outcome for a unit given the treatment assignment vector for its network neighborhood up to a user-specified distance, while marginalizing over treatment assignments for the rest of the network. Under the union of two Markov assumptions--one governing the propensity score model and the other the outcome model--we propose a corresponding semiparametric estimator based on general parametric specifications of nuisance functions. In particular, we suggest a class of parametric auto-regression models motivated by the Markov assumptions (Besag, 1974). By combining a restricted interference assumption with the propensity score model, we establish a new doubly robust identification result for the expected potential outcome under a hypothetical intervention on the treatment assignment vector for the entire network. We formally prove that, under weak network dependence, our proposed estimators are asymptotically normal and we characterize the impact of model misspecification on the asymptotic variance. Extensive simulation studies highlight the practical relevance of our approach. We further demonstrate its application in an empirical analysis of the NNAHRAY study, evaluating the impact of incarceration on individual socioeconomic outcomes in Brooklyn, New York.

---

## 43. Online selective conformal inference: adaptive scores, convergence rate and optimality

**作者**: Pierre Humbert, Ulysse Gazin, Ruth Heller, Etienne Roquain

**PDF链接**: [https://arxiv.org/pdf/2508.10336.pdf](https://arxiv.org/pdf/2508.10336.pdf)

**摘要**: In a supervised online setting, quantifying uncertainty has been proposed in the seminal work of \cite{gibbs2021adaptive}. For any given point-prediction algorithm, their method (ACI) produces a conformal prediction set with an average missed coverage getting close to a pre-specified level $\alpha$ for a long time horizon. We introduce an extended version of this algorithm, called OnlineSCI, allowing the user to additionally select times where such an inference should be made. OnlineSCI encompasses several prominent online selective tasks, such as building prediction intervals for extreme outcomes, classification with abstention, and online testing. While OnlineSCI controls the average missed coverage on the selected in an adversarial setting, our theoretical results also show that it controls the instantaneous error rate (IER) at the selected times, up to a non-asymptotical remainder term. Importantly, our theory covers the case where OnlineSCI updates the point-prediction algorithm at each time step, a property which we refer to as {\it adaptive} capability. We show that the adaptive versions of OnlineSCI can convergence to an optimal solution and provide an explicit convergence rate in each of the aforementioned application cases, under specific mild conditions. Finally, the favorable behavior of OnlineSCI in practice is illustrated by numerical experiments.

---

## 44. Multidimensional constructs and moderated linear and nonlinear factor analysis

**作者**: R. Noah Padgett

**PDF链接**: [https://arxiv.org/pdf/2509.05443.pdf](https://arxiv.org/pdf/2509.05443.pdf)

**摘要**: Multidimensional factor models with moderations on all model parameters have so far been limited to single-factor and two-factor models. This does not align well with existing psychological measures, which are commonly intended to assess 3-5 dimensions of a latent construct. In this paper, I introduce a multidimensional MNLFA model that permits the moderation of item intercepts, loadings, residual variances, factor means, variances, and correlations across three or more latent factors. I describe efforts to implement the model using Bayesian methods through Stan and penalized maximum likelihood approaches to stabilize estimation and detect partial measurement non-invariance while preserving model interpretability. Closed-form analytic gradients of the likelihood, eliminating the need for costly numerical or MCMC-based approximations. We conclude by discussing the theoretical implications of penalization for measurement invariance, computational considerations, and future directions for extending the framework to categorical indicators, longitudinal data, and applied research contexts.

---

## 45. Divide, Interact, Sample: The Two-System Paradigm

**作者**: James Chok, Myung Won Lee, Daniel Paulin, Geoffrey M. Vasil

**PDF链接**: [https://arxiv.org/pdf/2509.09162.pdf](https://arxiv.org/pdf/2509.09162.pdf)

**摘要**: Mean-field, ensemble-chain, and adaptive samplers have historically been viewed as distinct approaches to Monte Carlo sampling. In this paper, we present a unifying {two-system} framework that brings all three under one roof. In our approach, an ensemble of particles is split into two interacting subsystems that propose updates for each other in a symmetric, alternating fashion. This cross-system interaction ensures that the overall ensemble has $\rho(x)$ as its invariant distribution in both the finite-particle setting and the mean-field limit. The two-system construction reveals that ensemble-chain samplers can be interpreted as finite-$N$ approximations of an ideal mean-field sampler; conversely, it provides a principled recipe to discretize mean-field Langevin dynamics into tractable parallel MCMC algorithms. The framework also connects naturally to adaptive single-chain methods: by replacing particle-based statistics with time-averaged statistics from a single chain, one recovers analogous adaptive dynamics in the long-time limit without requiring a large ensemble. We derive novel two-system versions of both overdamped and underdamped Langevin MCMC samplers within this paradigm. Across synthetic benchmarks and real-world posterior inference tasks, these two-system samplers exhibit significant performance gains over the popular No-U-Turn Sampler, achieving an order of magnitude higher effective sample sizes per gradient evaluation.

---

## 46. A Spatio-temporal CP decomposition analysis of New England region in the US

**作者**: Fatoumata Sanogo

**PDF链接**: [https://arxiv.org/pdf/2510.10322.pdf](https://arxiv.org/pdf/2510.10322.pdf)

**摘要**: Spatio temporal data consist of measurement for one or more raster fields such as weather, traffic volume, crime rate, or disease incidents. Advances in modern technology have increased the number of available information for this type of data hence the rise of multidimensional data. In this paper we take advantage of the multidimensional structure of the data but also its temporal and spatial structure. In fact, we will be using the NCAR Climate Data Gateway website which provides data discovery and access services for global and regional climate model data. The daily values of total precipitation (prec), maximum (tmax), and minimum (tmin) temperature are combined to create a multidimensional data called tensor (a multidimensional array). In this paper, we propose a spatio temporal principal component analysis to initialize CP decomposition component. We take full advantage of the spatial and temporal structure of the data in the initialization step for cp component analysis. The performance of our method is tested via comparison with most popular initialization method. We also run a clustering analysis to further show the performance of our analysis.

---

## 47. Heterogeneity-Aware Federated Causal Inference Leveraging Effect-Measure Transportability

**作者**: Siqi Cao, Shu Yang

**PDF链接**: [https://arxiv.org/pdf/2510.16317.pdf](https://arxiv.org/pdf/2510.16317.pdf)

**摘要**: Federated learning of causal estimands offers a powerful strategy to improve estimation efficiency by leveraging data from multiple study sites while preserving privacy. Existing literature has primarily focused on the average treatment effect using single data source, whereas our work addresses a broader class of causal measures across multiple sources. We derive and compare semiparametrically efficient estimators under two transportability assumptions, which impose different restrictions on the data likelihood and illustrate the efficiency-robustness tradeoff. This estimator also permits the incorporation of flexible machine learning algorithms for nuisance functions while maintaining parametric convergence rates and nominal coverage. To further handle scenarios where some source sites violate transportability, we propose a Post-Federated Weighting Selection (PFWS) framework, which is a two-step procedure that adaptively identifies compatible sites and achieves the semiparametric efficiency bound asymptotically. This framework mitigates the efficiency loss of weighting methods and the instability and computational burden of direct site selection in finite samples. Through extensive simulations and real-data analysis, we demonstrate that our PFWS framework achieves superior variance efficiency compared with the target-only analyses across diverse transportability scenarios.

---

## 48. On Minimax Estimation Problems for Periodically Correlated Stochastic Processes

**作者**: Iryna Dubovets'ka, Mykhailo Moklyachuk

**PDF链接**: [https://arxiv.org/pdf/2510.16906.pdf](https://arxiv.org/pdf/2510.16906.pdf)

**摘要**: The aim of this article is to overview the problem of mean square optimal estimation of linear functionals which depend on unknown values of periodically correlated stochastic process. Estimates are based on observations of this process and noise. These problems are investigated under conditions of spectral certainty and spectral uncertainty. Formulas for calculating the main characteristics (spectral characteristic, mean square error) of the optimal linear estimates of the functionals are proposed. The least favorable spectral densities and the minimax-robust spectral characteristics of optimal estimates of the functionals are presented for given sets of admissible spectral densities.

---

## 49. Estimating Bidirectional Causal Effects with Large Scale Online Kernel Learning

**作者**: Masahiro Tanaka

**PDF链接**: [https://arxiv.org/pdf/2511.05050.pdf](https://arxiv.org/pdf/2511.05050.pdf)

**摘要**: In this study, a scalable online kernel learning framework is proposed for estimating bidirectional causal effects in systems characterized by mutual dependence and heteroskedasticity. Traditional causal inference often focuses on unidirectional effects, overlooking the common bidirectional relationships in real-world phenomena. Building on heteroskedasticity-based identification, the proposed method integrates a quasi-maximum likelihood estimator for simultaneous equation models with large scale online kernel learning. It employs random Fourier feature approximations to flexibly model nonlinear conditional means and variances, while an adaptive online gradient descent algorithm ensures computational efficiency for streaming and high-dimensional data. Results from extensive simulations demonstrate that the proposed method achieves superior accuracy and stability than single equation and polynomial approximation baselines, exhibiting lower bias and root mean squared error across various data-generating processes. These results confirm that the proposed approach effectively captures complex bidirectional causal effects with near-linear computational scaling. By combining econometric identification with modern machine learning techniques, the proposed framework offers a practical, scalable, and theoretically grounded solution for large scale causal inference in natural/social science, policy making, business, and industrial applications.

---

## 50. A unified approach to spatial domain detection and cell-type deconvolution in spot-based spatial transcriptomics

**作者**: Hyun Jung Koo, Aaron J. Molstad

**PDF链接**: [https://arxiv.org/pdf/2511.06204.pdf](https://arxiv.org/pdf/2511.06204.pdf)

**摘要**: Popular technologies for generating spatially resolved transcriptomic data measure gene expression at the resolution of a "spot", i.e., a small tissue region 55 microns in diameter. Each spot can contain many cells of different types. In typical analyses, researchers are interested in using these data to identify and profile discrete spatial domains in the tissue. In this paper, we propose a new method, DUET, that simultaneously identifies discrete spatial domains and estimates each spot's cell-type proportion. This allows the identified spatial domains to be characterized in terms of the cell type proportions, which affords interpretability and biological insight. DUET utilizes a constrained version of model-based convex clustering, and as such, can accommodate Poisson, negative binomial, normal, and other types of expression data. Through simulation studies and multiple applications, we show that DUET can achieve better clustering and deconvolution performance than existing methods.

---

## 51. Waiting for Dabo: A machine learning model for predicting Power 4 college football coaching hire success

**作者**: Michael Schuckers, Austin Hayes

**PDF链接**: [https://arxiv.org/pdf/2511.14035.pdf](https://arxiv.org/pdf/2511.14035.pdf)

**摘要**: Using data on 103 recent P4 college football hires, we built a statistical model for predicting a coach's success at their new school. For each hire, we collected data about their background and experiences, the previous success as a head coach or coordinator and their success since hiring. Over 50 variables on these factors were recorded though we used 29 of these in building our predictive model. Our measure of success is based upon Bill Connelly's SP+ team ratings relative to the performance on the same metric of the school in the 15 year prior to their selection as head coach. Using a cross-validated regularized linear regression, we obtain a predictive model for coaching success. Among the important factors for predicting a successful hire are having been a previous college head coach, leaving a job as an Offensive Coordinator, age and quality of the hiring school's team in the previous 15 years. While we do find these factors are important for the prediction of a successful coaching hire, the trends here are weak. With 66\% accuracy, the model does identify coaching hires that will outperform team performance in the 15 years before the hire. However, no combination of these factors leads to high predictability of identifying a successful coaching hire. All of the data and code for this paper are available in a Github repository.

---

## 52. New Empirical Process Tools and Their Applications to Robust Deep ReLU Networks and Phase Transitions for Nonparametric Regression

**作者**: Yizhe Ding, Runze Li, Lingzhou Xue

**PDF链接**: [https://arxiv.org/pdf/2511.15841.pdf](https://arxiv.org/pdf/2511.15841.pdf)

**摘要**: This paper introduces new empirical process tools for analyzing a broad class of statistical learning models under heavy-tailed noise and complex function classes. Our primary contribution is the derivation of two Dudley-type maximal inequalities for expected empirical processes that remove restrictive assumptions such as light tails and uniform boundedness of the function class. These inequalities enlarge the scope of empirical process theory available for statistical learning and nonparametric estimation. Exploiting the new bounds, we establish robustness guarantees for deep ReLU network estimators in Huber and quantile regression. In particular, we prove a unified non-asymptotic sub-Gaussian concentration bound that remains valid even under infinite-variance noise and provide a comprehensive analysis of non-asymptotic robustness for deep Huber estimators across all noise regimes. For deep quantile regression, we provide the first non-asymptotic sub-Gaussian bounds without requiring moment assumptions. As an additional application, our framework yields estimation error bounds for nonparametric least-squares estimators that simultaneously accommodate infinite-variance noise, non-Donsker function classes, and approximation error. Moreover, unlike prior approaches based on specialized multiplier processes, our framework extends to broader empirical risk minimization problems, including the nonparametric generalized linear models and the ``set-structured'' models.

---

## 53. Toward Super-polynomial Quantum Speedup of Equivariant Quantum Algorithms with SU($d$) Symmetry

**作者**: Han Zheng, Zimu Li, Sergii Strelchuk, Risi Kondor, Junyu Liu

**PDF链接**: [https://arxiv.org/pdf/2207.07250.pdf](https://arxiv.org/pdf/2207.07250.pdf)

**摘要**: We introduce a framework of the equivariant convolutional quantum algorithms which is tailored for a number of machine-learning tasks on physical systems with arbitrary SU$(d)$ symmetries. It allows us to enhance a natural model of quantum computation -- permutational quantum computing (PQC) -- and define a more powerful model: PQC+. While PQC was shown to be efficiently classically simulatable, we exhibit a problem which can be efficiently solved on PQC+ machine, whereas no classical polynomial time algorithm is known; thus providing evidence against PQC+ being classically simulatable. We further discuss practical quantum machine learning algorithms which can be carried out in the paradigm of PQC+.

---

## 54. A New Causal Rule Learning Approach to Interpretable Estimation of Heterogeneous Treatment Effect

**作者**: Ying Wu, Hanzhong Liu, Kai Ren, Shujie Ma, Xiangyu Chang

**PDF链接**: [https://arxiv.org/pdf/2310.06746.pdf](https://arxiv.org/pdf/2310.06746.pdf)

**摘要**: Interpretability plays a crucial role in the application of statistical learning to estimate heterogeneous treatment effects (HTE) in complex diseases. In this study, we leverage a rule-based workflow, namely causal rule learning (CRL), to estimate and improve our understanding of HTE for atrial septal defect, addressing an overlooked question in the previous literature: what if an individual simultaneously belongs to multiple groups with different average treatment effects? The CRL process consists of three steps: rule discovery, which generates a set of causal rules with corresponding subgroup average treatment effects; rule selection, which identifies a subset of these rules to deconstruct individual-level treatment effects as a linear combination of subgroup-level effects; and rule analysis, which presents a detailed procedure for further analyzing each selected rule from multiple perspectives to identify the most promising rules for validation. Extensive simulation studies and real-world data analysis demonstrate that CRL outperforms other methods in providing interpretable estimates of HTE, especially when dealing with complex ground truth and sufficient sample sizes.

---

## 55. A statistical method for crack pre-detection in 3D concrete images

**作者**: Vitalii Makogin, Duc Nguyen, Evgeny Spodarev

**PDF链接**: [https://arxiv.org/pdf/2402.16126.pdf](https://arxiv.org/pdf/2402.16126.pdf)

**摘要**: In practical applications, effectively segmenting cracks in large-scale computed tomography (CT) images holds significant importance for understanding the structural integrity of materials. Classical image-processing techniques and modern deep-learning models both face substantial computational challenges when applied directly to high resolution big data volumes. This paper introduces a statistical framework for crack pre-localization, whose purpose is not to replace or compete with segmentation networks, but to identify, with controlled error rates, the regions of a 3D CT image that are most likely to contain cracks. The method combines a simple Hessian-based filter, geometric descriptors computed on a regular spatial partition, and a spatial multiple testing procedure to detect anomalous regions while relying only on minimal calibration data, rather than large annotated datasets. Experiments on semi-synthetic and real 3D CT scans demonstrate that the proposed approach reliably highlights regions likely to contain cracks while preserving linear computational complexity. By restricting subsequent high resolution segmentation to these localized regions, deep-learning models can be trained and operate more efficiently, reducing both training runtime as well as resource consumption. The framework thus offers a practical and interpretable preprocessing step for large-scale CT inspection pipelines.

---

## 56. A Differentiable Alignment Framework for Sequence-to-Sequence Modeling via Optimal Transport

**作者**: Yacouba Kaloga, Shashi Kumar, Petr Motlicek, Ina Kodrasi

**PDF链接**: [https://arxiv.org/pdf/2502.01588.pdf](https://arxiv.org/pdf/2502.01588.pdf)

**摘要**: Accurate sequence-to-sequence (seq2seq) alignment is critical for applications like medical speech analysis and language learning tools relying on automatic speech recognition (ASR). State-of-the-art end-to-end (E2E) ASR systems, such as the Connectionist Temporal Classification (CTC) and transducer-based models, suffer from peaky behavior and alignment inaccuracies. In this paper, we propose a novel differentiable alignment framework based on one-dimensional optimal transport, enabling the model to learn a single alignment and perform ASR in an E2E manner. We introduce a pseudo-metric, called Sequence Optimal Transport Distance (SOTD), over the sequence space and discuss its theoretical properties. Based on the SOTD, we propose Optimal Temporal Transport Classification (OTTC) loss for ASR and contrast its behavior with CTC. Experimental results on the TIMIT, AMI, and LibriSpeech datasets show that our method considerably improves alignment performance compared to CTC and the more recently proposed Consistency-Regularized CTC, though with a trade-off in ASR performance. We believe this work opens new avenues for seq2seq alignment research, providing a solid foundation for further exploration and development within the community. Our code is publicly available at: this https URL

---

## 57. Analyzing distortion riskmetrics and weighted entropy for unimodal and symmetric distributions under partial information constraints

**作者**: Baishuai Zuo, Chuancun Yin

**PDF链接**: [https://arxiv.org/pdf/2504.19725.pdf](https://arxiv.org/pdf/2504.19725.pdf)

**摘要**: In this paper, we develop the lower and upper bounds of worst-case distortion riskmetrics and weighted entropy for unimodal, and symmetric unimodal distributions when mean and variance information are available. We also consider the sharp upper bounds of distortion riskmetrics and weighted entropy for symmetric distribution under known mean and variance. These results are applied to (weighted) entropies, shortfalls and other risk measures. Specifically, entropies include cumulative Tsallis past entropy, cumulative residual Tsallis entropy of order {\alpha}, extended Gini coefficient, fractional generalized cumulative residual entropy, and fractional generalized cumulative entropy. Shortfalls include extended Gini shortfall, Gini shortfall, shortfall of cumulative residual entropy, and shortfall of cumulative residual Tsallis entropy. Other risk measures include nth-order expected shortfall, dual power principle and proportional hazard principle.

---

## 58. Deterministic Bounds and Random Estimates of Metric Tensors on Neuromanifolds

**作者**: Ke Sun

**PDF链接**: [https://arxiv.org/pdf/2505.13614.pdf](https://arxiv.org/pdf/2505.13614.pdf)

**摘要**: The high dimensional parameter space of modern deep neural networks -- the neuromanifold -- is endowed with a unique metric tensor defined by the Fisher information, estimating which is crucial for both theory and practical methods in deep learning. To analyze this tensor for classification networks, we return to a low dimensional space of probability distributions -- the core space -- and carefully analyze the spectrum of its Riemannian metric. We extend our discoveries there into deterministic bounds of the metric tensor on the neuromanifold. We introduce an unbiased random estimate of the metric tensor and its bounds based on Hutchinson's trace estimator. It can be evaluated efficiently through a single backward pass, with a standard deviation bounded by the true value up to scaling.

---

## 59. The Impact of Feature Scaling In Machine Learning: Effects on Regression and Classification Tasks

**作者**: João Manoel Herrera Pinheiro, Suzana Vilas Boas de Oliveira, Thiago Henrique Segreto Silva, Pedro Antonio Rabelo Saraiva, Enzo Ferreira de Souza, Ricardo V. Godoy, Leonardo André Ambrosio, Marcelo Becker

**PDF链接**: [https://arxiv.org/pdf/2506.08274.pdf](https://arxiv.org/pdf/2506.08274.pdf)

**摘要**: This research addresses the critical lack of comprehensive studies on feature scaling by systematically evaluating 12 scaling techniques - including several less common transformations - across 14 different Machine Learning algorithms and 16 datasets for classification and regression tasks. We meticulously analyzed impacts on predictive performance (using metrics such as accuracy, MAE, MSE, and $R^2$) and computational costs (training time, inference time, and memory usage). Key findings reveal that while ensemble methods (such as Random Forest and gradient boosting models like XGBoost, CatBoost and LightGBM) demonstrate robust performance largely independent of scaling, other widely used models such as Logistic Regression, SVMs, TabNet, and MLPs show significant performance variations highly dependent on the chosen scaler. This extensive empirical analysis, with all source code, experimental results, and model parameters made publicly available to ensure complete transparency and reproducibility, offers model-specific crucial guidance to practitioners on the need for an optimal selection of feature scaling techniques.

---

## 60. Manifolds with kinks and the asymptotic behavior of the graph Laplacian operator with Gaussian kernel

**作者**: Susovan Pal, David Tewodrose

**PDF链接**: [https://arxiv.org/pdf/2507.07751.pdf](https://arxiv.org/pdf/2507.07751.pdf)

**摘要**: We introduce manifolds with kinks, a class of manifolds with possibly singular boundary that notably contains manifolds with smooth boundary and corners. We derive the asymptotic behavior of the Graph Laplace operator with Gaussian kernel and its deterministic limit on these spaces as bandwidth goes to zero. We show that this asymptotic behavior is determined by the inward sector of the tangent space and, as special cases, we derive its behavior near interior and singular points. Lastly, we show the validity of our theoretical results using numerical simulation.

---

## 61. ResCP: Reservoir Conformal Prediction for Time Series Forecasting

**作者**: Roberto Neglia, Andrea Cini, Michael M. Bronstein, Filippo Maria Bianchi

**PDF链接**: [https://arxiv.org/pdf/2510.05060.pdf](https://arxiv.org/pdf/2510.05060.pdf)

**摘要**: Conformal prediction offers a powerful framework for building distribution-free prediction intervals for exchangeable data. Existing methods that extend conformal prediction to sequential data rely on fitting a relatively complex model to capture temporal dependencies. However, these methods can fail if the sample size is small and often require expensive retraining when the underlying data distribution changes. To overcome these limitations, we propose Reservoir Conformal Prediction (ResCP), a novel training-free conformal prediction method for time series. Our approach leverages the efficiency and representation learning capabilities of reservoir computing to dynamically reweight conformity scores. In particular, we compute similarity scores among reservoir states and use them to adaptively reweight the observed residuals at each step. With this approach, ResCP enables us to account for local temporal dynamics when modeling the error distribution without compromising computational scalability. We prove that, under reasonable assumptions, ResCP achieves asymptotic conditional coverage, and we empirically demonstrate its effectiveness across diverse forecasting tasks.

---

## 62. Generalization Bounds for Semi-supervised Matrix Completion with Distributional Side Information

**作者**: Antoine Ledent, Mun Chong Soo, Nong Minh Hieu

**PDF链接**: [https://arxiv.org/pdf/2511.13049.pdf](https://arxiv.org/pdf/2511.13049.pdf)

**摘要**: We study a matrix completion problem where both the ground truth $R$ matrix and the unknown sampling distribution $P$ over observed entries are low-rank matrices, and \textit{share a common subspace}. We assume that a large amount $M$ of \textit{unlabeled} data drawn from the sampling distribution $P$ is available, together with a small amount $N$ of labeled data drawn from the same distribution and noisy estimates of the corresponding ground truth entries. This setting is inspired by recommender systems scenarios where the unlabeled data corresponds to `implicit feedback' (consisting in interactions such as purchase, click, etc. ) and the labeled data corresponds to the `explicit feedback', consisting of interactions where the user has given an explicit rating to the item. Leveraging powerful results from the theory of low-rank subspace recovery, together with classic generalization bounds for matrix completion models, we show error bounds consisting of a sum of two error terms scaling as $\widetilde{O}\left(\sqrt{\frac{nd}{M}}\right)$ and $\widetilde{O}\left(\sqrt{\frac{dr}{N}}\right)$ respectively, where $d$ is the rank of $P$ and $r$ is the rank of $M$. In synthetic experiments, we confirm that the true generalization error naturally splits into independent error terms corresponding to the estimations of $P$ and and the ground truth matrix $\ground$ respectively. In real-life experiments on Douban and MovieLens with most explicit ratings removed, we demonstrate that the method can outperform baselines relying only on the explicit ratings, demonstrating that our assumptions provide a valid toy theoretical setting to study the interaction between explicit and implicit feedbacks in recommender systems.

---

## 63. Sensor Informativeness, Identifiability, and Uncertainty in Bayesian Inverse Problems for Structural Health Monitoring

**作者**: Tammam Bakeer, Max Herbers, Steffen Marx

**PDF链接**: [https://arxiv.org/pdf/2511.16628.pdf](https://arxiv.org/pdf/2511.16628.pdf)

**摘要**: In Structural Health Monitoring (SHM), the recovery of distributed mechanical parameters from sparse data is often ill-posed, raising critical questions about identifiability and the reliability of inferred states. While deterministic regularization methods such as Tikhonov stabilise the inversion, they provide little insight into the spatial limits of resolution or the inherent uncertainty of the solution. This paper presents a Bayesian inverse framework that rigorously quantifies these limits, using the identification of distributed flexural rigidity from rotation (tilt) influence lines as a primary case study. Fisher information is employed as a diagnostic metric to quantify sensor informativeness, revealing how specific sensor layouts and load paths constrain the recoverable spatial features of the parameter field.
The methodology is applied to the full-scale openLAB research bridge (TU Dresden) using data from controlled vehicle passages. Beyond estimating the flexural rigidity profile, the Bayesian formulation produces credible intervals that expose regions of practical non-identifiability, which deterministic methods may obscure. The results demonstrate that while the measurement data carry high information content for the target parameters, their utility is spatially heterogeneous and strictly bounded by the experiment design. The proposed framework unifies identification with uncertainty quantification, providing a rigorous basis for optimising sensor placement and interpreting the credibility of SHM diagnostics.

---

